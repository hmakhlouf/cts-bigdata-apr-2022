{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f4ba16-451b-4fcf-b843-15f3e43a0bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# findspark\n",
    "# finds spark installation in the system,  using SPARK_HOME\n",
    "# automatically set path, environment needed for pyspark\n",
    "# load spark libraries etc\n",
    "# /opt/spark3.1.2.......\n",
    "# good for single machine development, learning\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41831b17-bd46-41d6-b68d-520610efcbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/05 00:01:16 WARN Utils: Your hostname, ubuntu-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.174.129 instead (on interface ens33)\n",
      "22/05/05 00:01:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/05/05 00:01:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/05 00:01:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Context - helps to create rdd, dag, job, task execute task etc, Spark context is SPARK CORE\n",
    "# this code is called spark application, or spark driver\n",
    "# every spark driver shall have ONLY ONE spark context\n",
    "from pyspark import SparkContext\n",
    "# local is execution mode, spark driver, \n",
    "# spark executor runs in same JVM in same machine / not distributed\n",
    "# good for development, learning , not for production\n",
    "# SparkBasic is application name of your choice\n",
    "sc = SparkContext(\"local\", \"SparkBasic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af189eb-4e83-4744-993a-5cb04f6adc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RDD, from hardcoded data\n",
    "# data is hardcoded in Spark Driver, this notebook\n",
    "# RDD shall be created in Executor process\n",
    "# Lazy evaluation \n",
    "# intellisense - editor/notebook automatically brings up functions as you type\n",
    "# sc.<TAB><TAB><TAB> - will bring up all functions\n",
    "# creating RDD using parallelize method, by loading hardcoded data\n",
    "# RDD shall have partition(s)\n",
    "# the data hardcoded shall be loaded into partitions\n",
    "# at this moment, no data shall be loaded, as this is lazy loading\n",
    "# when we apply action only then data shall be loaded\n",
    "# create and return RDD\n",
    "rdd = sc.parallelize([0,1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "766361c1-b157-4443-a88a-1e328c4469bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply filter operation, we call this as TRANSFORMATION\n",
    "# TRANSFORMATION is code/task applied on partitioned data\n",
    "# filter is higher order function, accept a function as input\n",
    "# filter apply data (n) from partition to function supplied lambda n: n % 2 == 1\n",
    "# lambda n: n % 2 == 1 returns either true or false,\n",
    "# filter collect all the numbers where fitler return true 1, 3, 5, 7, 9\n",
    "# LAZY Evaluation, no partition, no data , not code loaded into Exeuctor\n",
    "# until we apply ACTION on RDD\n",
    "# lambda n: n % 2 == 1 shall be executed by executor\n",
    "# spark driver shall send lambda n: n % 2 == 1 code to executor called task\n",
    "oddRdd = rdd.filter (lambda n: n % 2 == 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b054e9cb-30af-42b4-8f04-1ec48733941c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# collect is an ACTION method\n",
    "# Every ACTION Methods create JOB\n",
    "# JOB is split into STAGES\n",
    "# Each STAGE shall have TASKs\n",
    "# TASKs shall be running on Executor on PARTITION\n",
    "# Finally, collect bring the output back to DRIVER from EXECUTORs\n",
    "# Action is the one will create and distribute partitions, run tasks on executors\n",
    "results = oddRdd.collect()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c8b1c40-8b7d-4ac0-bdd0-54bfebacadae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# min is an ACTION,\n",
    "# this create job, stages, DAG, tasks execute on cluster independently \n",
    "r = oddRdd.min ()\n",
    "print(r) # print min of odd number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84575569-b89d-412b-9f65-8016060da0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "r2 = oddRdd.max()\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c4bcf89-5f22-4a52-88f5-97ca87ee93a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "r3 = oddRdd.mean()\n",
    "print(r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e81af6d-9e46-4366-b5c4-ed08e7ac9b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "r4 = oddRdd.sum()\n",
    "print(r4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61acbd5-3ea1-4930-bc09-5505031593f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
